---
title: "Entrenamiento"
author: "Andres Garcia"
date: "25/11/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r message=FALSE, warning=FALSE, results = 'hide', echo=F}
# Librerias

library(stringr) 
library(dplyr) 
library(rgdal) 
library(tidyverse) 
library(kableExtra) 
library(lubridate)
library(ggplot2) 
library(plotly) 
library(ggpubr)
library(readxl)
library(sf)
library(GGally)
library(car)
library(MLmetrics)
library(wordcloud)
library(gplots)
library(R.utils)
library(tm)
library(DescTools)
library(raster)
library(mclust)
library(geosphere)
library(NbClust)
library(factoextra)
library(vegan)
library(qpcR)
library(leaflet)
library(stringi)
library(ggplot2)
library(cluster)
```


## 3. Entrenamiento de un modelo predictivo

<div style="text-align: justify">
En esta sección. construiremos y validaremos un modelo que permita predecir la accidentalidad por tipo de accidente a nivel semanal, mensual y diario. Para esto se consideran fechas especiales.

Los modelos predictivos que veremos se construirán con los datos de los años 2014, 2015, 2016, 2017 y 2018; esta será la base para entrenamiento. Los accidentes del año 2019 y 2020 se usarán para validar los modelos.

El criterio de éxito de los modelos predictivos será el MAE de la predicción.
</div>

```{r message=FALSE, warning=FALSE, results = 'hide', echo=F}
# Lectura de la base de datos
bd_depurada <- read.csv("base_depurada3.csv", dec=".", header=T,sep=",")
bd_depurada$CLASE <- as.factor(as.character(bd_depurada$CLASE))

# Division en train y val
datos_val_2019 <- subset(bd_depurada, (ANO == '2019'))
datos_val_2020 <- subset(bd_depurada, (ANO == '2020'))
base_train <- subset(bd_depurada, (ANO != '2019' & ANO != '2020'))

```

### 3.1 Diaria
<div style="text-align: justify">
Empezaremos por buscar el mejor modelo para realizar las predicciones diarias.
</div>

### 3.1.1 Modelo 1: modelo lineal generalizado inicial

<div style="text-align: justify">
Como nos interesa predecir el número de accidentes por unidad de tiempo, resulta conveniente utilizar un modelo lineal generalizado con la distribución Poisson. Para este primer modelo, consideraremos únicamente las variables `FESTIVIDAD` Y `DIA_SEMANA` para predecir la accidentalidad.
</div>

```{r message=FALSE, warning=FALSE, results = 'hide', echo=F}
#Modelo lineal
datos_lm1_tr <- base_train %>% group_by(FECHA, FESTIVIDAD, 
                                     DIA_SEMANA) %>% count(name = "NRO_ACCID") 

# lm1 <- lm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA, data = datos_lm1_tr)
lm1 <- glm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA, family = "poisson", data = datos_lm1_tr)
```

### 3.1.1.2 Predicción y Evaluación para los datos de Entrenamiento

```{r message=FALSE, warning=FALSE, echo=F}
### Evaluación modelo 1
datos_lm1_tr_pred <- datos_lm1_tr[,-c(4)]
predicted <- round(predict(lm1, newdata=datos_lm1_tr_pred, type="response"))
actual <- datos_lm1_tr$NRO_ACCID
lm1_tr_mse <- MSE(predicted, actual) # MSE
lm1_tr_mae <- MAE(predicted, actual) # MAE
lm1_tr_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm1_tr_mse, lm1_tr_mae, lm1_tr_r2)
```

<div style="text-align: justify">
Para los datos de entrenamiento, se obtiene un MAE de `r lm1_tr_mae`, y un R2 de `r lm1_tr_r2`.
</div>

### 3.1.1.3 Predicción y Evaluación para los datos de Validación en el año 2019

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm1_val <- datos_val_2019 %>% group_by(FECHA, FESTIVIDAD, 
                                        DIA_SEMANA) %>% count(name = "NRO_ACCID")

datos_lm1_val_pred <- datos_lm1_val[,-c(4)]
predicted <- round(predict(lm1, newdata=datos_lm1_val_pred, type="response"))
actual <- datos_lm1_val$NRO_ACCID
lm1_val_mse <- MSE(predicted, actual) # MSE
lm1_val_mae <- MAE(predicted, actual) # MAE
lm1_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm1_val_mse, lm1_val_mae, lm1_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2019, se obtiene un MAE de `r lm1_val_mae`, y un R2 de `r lm1_val_r2`.
</div>

### 3.1.1.4 Comparación entre las métricas de entrenamiento y validación (2019)

```{r message=FALSE, warning=FALSE, echo=F}
mae_variation = function(mae_tr, mae_val){
  variacion = mae_tr - mae_vl
  porcentaje = variacion / mae_tr
  porcentaje * 100 * -1
}

mae_tr = lm1_tr_mae
mae_vl = lm1_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación es de `r resultado`%, lo cual indica un posible sobre entrenamiento. Además, el R2 es relativamente bajo, cercano al 50%. Veamos que pasa para el año 2020.
</div>

### 3.1.1.5 Predicción y Evaluación para los datos de Validación en el año 2020

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm1_val <- datos_val_2020 %>% group_by(FECHA, FESTIVIDAD, 
                                        DIA_SEMANA) %>% count(name = "NRO_ACCID")

datos_lm1_val_pred <- datos_lm1_val[,-c(4)]
predicted <- round(predict(lm1, newdata=datos_lm1_val_pred, type="response"))
actual <- datos_lm1_val$NRO_ACCID
lm1_val_mse <- MSE(predicted, actual) # MSE
lm1_val_mae <- MAE(predicted, actual) # MAE
lm1_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm1_val_mse, lm1_val_mae, lm1_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2020, se obtiene un MAE de `r lm1_val_mae`, y un R2 de `r lm1_val_r2`. Este MAE tan alto y este R2 negativo indican que el modelo se ajusta muy pobremente a los datos del año 2020. 

Sin embargo, tal como veremos a continuación, ningún modelo se ajusta bien al año 2020. Esto se puede explicar por dos posibles razones:

1. En 2020 fue el inicio de la pandemia, y hubo muchos menos accidentes.
2. En 2020 solo hay observaciones hasta el mes de agosto.

Por tanto, el año 2020 no nos será muy útil para validar los modelos, ya que el comportamiento de este año es muy diferente a los demás años con los que se entrenó el modelo.
</div>

### 3.1.1.6 Comparación entre las métricas de entrenamiento y validación (2020)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm1_tr_mae
mae_vl = lm1_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación es de `r resultado`%.
</div>

### 3.1.2 Modelo 2: modelo lineal generalizado, usando la variable clase

<div style="text-align: justify">
En el segundo modelo, utilizaremos las mismas variables del modelo inicial, y sumaremos la variable `CLASE`. Veamos su desempeño.
</div>

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm2_tr <- base_train %>% group_by(FECHA, FESTIVIDAD, DIA_SEMANA, 
                                   CLASE) %>% count(name = "NRO_ACCID")
# lm2 <- lm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+CLASE,
#            data = datos_lm2_tr)
lm2 <- glm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+CLASE, family = "poisson",
           data = datos_lm2_tr)
```

### 3.1.2.1 Predicción y Evaluación para los datos de Entrenamiento

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm2_tr_pred <- datos_lm2_tr[,-c(5)]
predicted <- round(predict(lm2, newdata=datos_lm2_tr_pred, type="response"))
actual <- datos_lm2_tr$NRO_ACCID
lm2_tr_mse <- MSE(predicted, actual) # MSE
lm2_tr_mae <- MAE(predicted, actual) # MAE
lm2_tr_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm2_tr_mse, lm2_tr_mae, lm2_tr_r2)
```

<div style="text-align: justify">
Para los datos de entrenamiento, se obtiene un MAE de `r lm2_tr_mae`, y un R2 de `r lm2_tr_r2`.
</div>

### 3.1.2.2 Predicción y Evaluación para los datos de Validación en el año 2019

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm2_val <- datos_val_2019 %>% group_by(FECHA, FESTIVIDAD, 
                                        DIA_SEMANA, CLASE) %>% count(name = "NRO_ACCID")

datos_lm2_val_pred <- datos_lm2_val[,-c(5)]
predicted <- round(predict(lm2, newdata=datos_lm2_val_pred, type="response"))
actual <- datos_lm2_val$NRO_ACCID
lm2_val_mse <- MSE(predicted, actual) # MSE
lm2_val_mae <- MAE(predicted, actual) # MAE
lm2_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm2_val_mse, lm2_val_mae, lm2_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2019, se obtiene un MAE de `r lm2_val_mae`, y un R2 de `r lm2_val_r2`. Estos valores son mucho mejores que los obtenidos con el modelo anterior.
</div>

### 3.1.2.3 Comparación entre las métricas de entrenamiento y validación (2019)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm2_tr_mae
mae_vl = lm2_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación es de `r resultado`%; según esta cifra, no hay indicios de sobre entrenamiento. El R2 fue muy bueno tanto en entrenamiento como en validación, superior al 90%. Este modelo es un muy buen candidato para ser utilizado en las predicciones futuras.
</div>

### 3.1.2.4 Predicción y Evaluación para los datos de Validación en el año 2020

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm2_val <- datos_val_2020 %>% group_by(FECHA, FESTIVIDAD, 
                                        DIA_SEMANA, CLASE) %>% count(name = "NRO_ACCID")

datos_lm2_val_pred <- datos_lm2_val[,-c(5)]
predicted <- round(predict(lm2, newdata=datos_lm2_val_pred, type="response"))
actual <- datos_lm2_val$NRO_ACCID
lm2_val_mse <- MSE(predicted, actual) # MSE
lm2_val_mae <- MAE(predicted, actual) # MAE
lm2_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm2_val_mse, lm2_val_mae, lm2_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2020, se obtiene un MAE de `r lm2_val_mae`, y un R2 de `r lm2_val_r2`. El modelo se ajusta ligeramente mejor a los datos de 2020 respecto al caso anterior, pero sigue siendo muy inadecuado para predecir la accidentalidad de este año. Tal como se explicó anteriormente, no es adecuado validar el modelo con estos datos.
</div>

### 3.1.2.5 Comparación entre las métricas de entrenamiento y validación (2020)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm2_tr_mae
mae_vl = lm2_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación es de `r resultado`%.
</div>

### 3.1.3 Modelo 3: modelo lineal generalizado, usando la variable diseño

<div style="text-align: justify">
El modelo 2 tuvo un muy buen desempeño. Aun así, sería interesante probar modelos utilizando otras variables. Para este caso usaremos las variables `Festividad`, `Día Semana` y `Diseño`.
</div>

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm3_tr <- base_train %>% group_by(FECHA, FESTIVIDAD, DIA_SEMANA, 
                                   DISENO) %>% count(name = "NRO_ACCID")
# lm3 <- lm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+DISENO,
#            data = datos_lm3_tr)
lm3 <- glm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+DISENO, family = "poisson",
           data = datos_lm3_tr)
```

### 3.1.3.1 Predicción y Evaluación para los datos de Entrenamiento

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm3_tr_pred <- datos_lm3_tr[,-c(5)]
predicted <- round(predict(lm3, newdata=datos_lm3_tr_pred, type="response"))
actual <- datos_lm3_tr$NRO_ACCID
lm3_tr_mse <- MSE(predicted, actual) # MSE
lm3_tr_mae <- MAE(predicted, actual) # MAE
lm3_tr_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm3_tr_mse, lm3_tr_mae, lm3_tr_r2)
```

<div style="text-align: justify">
Para los datos de entrenamiento, se obtiene un MAE de `r lm3_tr_mae`, y un R2 de `r lm3_tr_r2`.
</div>

### 3.1.3.2 Predicción y Evaluación para los datos de Validación en el año 2019

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm3_val <- datos_val_2019 %>% group_by(FECHA, FESTIVIDAD, 
                                        DIA_SEMANA, DISENO) %>% count(name = "NRO_ACCID")

datos_lm3_val_pred <- datos_lm3_val[,-c(5)]
predicted <- round(predict(lm3, newdata=datos_lm3_val_pred, type="response"))
actual <- datos_lm3_val$NRO_ACCID
lm3_val_mse <- MSE(predicted, actual) # MSE
lm3_val_mae <- MAE(predicted, actual) # MAE
lm3_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm3_val_mse, lm3_val_mae, lm3_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2019, se obtiene un MAE de `r lm3_val_mae`, y un R2 de `r lm3_val_r2`. 
</div>

### 3.1.3.3 Comparación entre las métricas de entrenamiento y validación (2019)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm3_tr_mae
mae_vl = lm3_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación fue de `r resultado`%; según esta cifra, no hay indicios de sobre entrenamiento. El R2 fue bastante bueno tanto en entrenamiento como en validación, cercano al 90%, pero inferior al del modelo 2. Además, el MAE también fue superior que el del modelo 2. Por tanto, descartamos este modelo.
</div>

### 3.1.3.4 Predicción y Evaluación para los datos de Validación en el año 2020

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm3_val <- datos_val_2020 %>% group_by(FECHA, FESTIVIDAD, 
                                        DIA_SEMANA, DISENO) %>% count(name = "NRO_ACCID")

datos_lm3_val_pred <- datos_lm3_val[,-c(5)]
predicted <- round(predict(lm3, newdata=datos_lm3_val_pred, type="response"))
actual <- datos_lm3_val$NRO_ACCID
lm3_val_mse <- MSE(predicted, actual) # MSE
lm3_val_mae <- MAE(predicted, actual) # MAE
lm3_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm3_val_mse, lm3_val_mae, lm3_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2020, se obtiene un MAE de `r lm3_val_mae`, y un R2 de `r lm3_val_r2`.
</div>

### 3.1.3.5 Comparación entre las métricas de entrenamiento y validación (2020)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm3_tr_mae
mae_vl = lm3_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación es de `r resultado`%.
</div>

### 3.1.4 Modelo 4: modelo lineal generalizado, usando la variable comuna

<div style="text-align: justify">
Ahora, probaremos un nuevo modelo, tomando las mismas variables del modelo inicial, pero añadiendo la variable `COMUNA`.
</div>

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm5_tr <- base_train %>% group_by(FECHA, FESTIVIDAD, DIA_SEMANA, 
                                   COMUNA) %>% count(name = "NRO_ACCID")
# lm5 <- lm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+COMUNA,
#            data = datos_lm5_tr)
lm5 <- glm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+COMUNA, family = "poisson",
           data = datos_lm5_tr)
```

### 3.1.4.1 Predicción y Evaluación para los datos de Entrenamiento

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm5_tr_pred <- datos_lm5_tr[,-c(5)]
predicted <- round(predict(lm5, newdata=datos_lm5_tr_pred, type="response"))
actual <- datos_lm5_tr$NRO_ACCID
lm5_tr_mse <- MSE(predicted, actual) # MSE
lm5_tr_mae <- MAE(predicted, actual) # MAE
lm5_tr_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm5_tr_mse, lm5_tr_mae, lm5_tr_r2)
```

<div style="text-align: justify">
Para los datos de entrenamiento, se obtiene un MAE de `r lm5_tr_mae`, y un R2 de `r lm5_tr_r2`.
</div>

### 3.1.4.2 Predicción y Evaluación para los datos de Validación en el año 2019

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm5_val <- datos_val_2019 %>% group_by(FECHA, FESTIVIDAD, 
                                        DIA_SEMANA, COMUNA) %>% count(name = "NRO_ACCID")

datos_lm5_val_pred <- datos_lm5_val[,-c(5)]
predicted <- round(predict(lm5, newdata=datos_lm5_val_pred, type="response"))
actual <- datos_lm5_val$NRO_ACCID
lm5_val_mse <- MSE(predicted, actual) # MSE
lm5_val_mae <- MAE(predicted, actual) # MAE
lm5_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm5_val_mse, lm5_val_mae, lm5_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2019, se obtiene un MAE de `r lm5_val_mae`, y un R2 de `r lm5_val_r2`. 
</div>

### 3.1.4.3 Comparación entre las métricas de entrenamiento y validación (2019)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm5_tr_mae
mae_vl = lm5_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación fue de `r resultado`%; esta es la variación más baja obtenida hasta el momento, y nos da una buena señal de que no hay sobre entrenamiento. También, a pesar de que los R2 son ligeramente inferiores respecto a los modelos 2 y 3, pues son cercanos al 80%, el MAE fue mucho más bajo que en los anteriores modelos, y, teniendo en cuenta que el MAE es nuestro criterio de éxito, podemos decir que este modelo también es un buen candidato para realizar nuestras predicciones.
</div>

### 3.1.4.4 Predicción y Evaluación para los datos de Validación en el año 2020

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm5_val <- datos_val_2020 %>% group_by(FECHA, FESTIVIDAD, 
                                        DIA_SEMANA, COMUNA) %>% count(name = "NRO_ACCID")

datos_lm5_val_pred <- datos_lm5_val[,-c(5)]
predicted <- round(predict(lm5, newdata=datos_lm5_val_pred, type="response"))
actual <- datos_lm5_val$NRO_ACCID
lm5_val_mse <- MSE(predicted, actual) # MSE
lm5_val_mae <- MAE(predicted, actual) # MAE
lm5_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm5_val_mse, lm5_val_mae, lm5_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2020, se obtiene un MAE de `r lm5_val_mae`, y un R2 de `r lm5_val_r2`.
</div>

### 3.1.4.5 Comparación entre las métricas de entrenamiento y validación (2020)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm5_tr_mae
mae_vl = lm5_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación es de `r resultado`%.
</div>

### 3.1.5 Modelo 5: modelo lineal generalizado, usando las variables clase y comuna

<div style="text-align: justify">
Finalmente, probaremos un último modelo, usando aquellas variables que tuvieron el mejor MAE en los modelos anteriores, es decir, `CLASE` y `COMUNA`. Se incluirán también las variables del modelo inicial.
</div>

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_tr <- base_train %>% group_by(FECHA, FESTIVIDAD, DIA_SEMANA, 
                                   CLASE, COMUNA) %>% count(name = "NRO_ACCID")
# lm6 <- lm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+CLASE+COMUNA,
#            data = datos_lm6_tr)
lm6 <- glm(NRO_ACCID ~ FESTIVIDAD+f+CLASE+COMUNA, family = "poisson",
           data = datos_lm6_tr)
```

```{r}

```

```{r}
save(file="lm6.RData",lm6)
```

### 3.1.5.1 Predicción y Evaluación para los datos de Entrenamiento

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_tr_pred <- datos_lm6_tr[,-c(6)]
predicted <- round(predict(lm6, newdata=datos_lm6_tr_pred, type="response"))
actual <- datos_lm6_tr$NRO_ACCID
lm6_tr_mse <- MSE(predicted, actual) # MSE
lm6_tr_mae <- MAE(predicted, actual) # MAE
lm6_tr_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_tr_mse, lm6_tr_mae, lm6_tr_r2)
```

<div style="text-align: justify">
Para los datos de entrenamiento, se obtiene un MAE de `r lm6_tr_mae`, y un R2 de `r lm6_tr_r2`.
</div>

### 3.1.5.2 Predicción y Evaluación para los datos de Validación en el año 2019

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_val <- datos_val_2019 %>% group_by(FECHA, FESTIVIDAD, DIA_SEMANA, 
                                        CLASE, COMUNA) %>% count(name = "NRO_ACCID")

datos_lm6_val_pred <- datos_lm6_val[,-c(6)]
predicted <- round(predict(lm6, newdata=datos_lm6_val_pred, type="response"))
actual <- datos_lm6_val$NRO_ACCID
lm6_val_mse <- MSE(predicted, actual) # MSE
lm6_val_mae <- MAE(predicted, actual) # MAE
lm6_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_val_mse, lm6_val_mae, lm6_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2019, se obtiene un MAE de `r lm6_val_mae`, y un R2 de `r lm6_val_r2`. 
</div>

### 3.1.5.3 Comparación entre las métricas de entrenamiento y validación (2019)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm6_tr_mae
mae_vl = lm6_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación fue mínima, de tan solo `r resultado`%; este dato nos ayuda a estar muy seguros de que no hay sobre entrenamiento. Con este modelo también conseguimos el MAE más pequeño de todos, cercano a 1, y el R2 cuadrado sigue siendo bueno, superior al 70%. Por tanto, concluimos que este es el mejor modelo para predecir, según nuestro criterio de éxito.
</div>

### 3.1.5.4 Predicción y Evaluación para los datos de Validación en el año 2020

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_val <- datos_val_2020 %>% group_by(FECHA, FESTIVIDAD, DIA_SEMANA, 
                                        CLASE, COMUNA) %>% count(name = "NRO_ACCID")

datos_lm6_val_pred <- datos_lm6_val[,-c(6)]
predicted <- round(predict(lm6, newdata=datos_lm6_val_pred, type="response"))
actual <- datos_lm6_val$NRO_ACCID
lm6_val_mse <- MSE(predicted, actual) # MSE
lm6_val_mae <- MAE(predicted, actual) # MAE
lm6_val_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_val_mse, lm6_val_mae, lm6_val_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2020, se obtiene un MAE de `r lm6_val_mae`, y un R2 de `r lm6_val_r2`.
</div>

### 3.1.5.5 Comparación entre las métricas de entrenamiento y validación (2020)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm6_tr_mae
mae_vl = lm6_val_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación fue de `r resultado`%.
</div>

### 3.2 Semanal

<div style="text-align: justify">
Una vez tenemos determinado el mejor modelo (5) para las predicciones diarias, podemos pasar a evaluarlo semanalmente para validar su eficiencia en plazos semanales.
</div>

### 3.2.1 Modelo seleccionado: modelo lineal generalizado, usando las variables clase y comuna

<div style="text-align: justify">
Veamos cómo se comporta este modelo para predecir la accidentalidad semanalmente. En este caso, las variables a utilizar serán `FESTIVIDAD`, `SEMANA`, `CLASE` y `COMUNA`.
</div>

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_tr_semanal <- base_train %>% group_by(FECHA, FESTIVIDAD, SEMANA, 
                                   CLASE, COMUNA) %>% count(name = "NRO_ACCID")
# lm6 <- lm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+CLASE+COMUNA,
#            data = datos_lm6_tr)
lm6_semanal <- glm(NRO_ACCID ~ FESTIVIDAD+SEMANA+CLASE+COMUNA, family = "poisson",
           data = datos_lm6_tr_semanal)
```

### 3.2.1.1 Predicción y Evaluación para los datos de Entrenamiento

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_tr_pred_semanal <- datos_lm6_tr_semanal[,-c(6)]
predicted <- round(predict(lm6_semanal, newdata=datos_lm6_tr_pred_semanal, type="response"))
actual <- datos_lm6_tr_semanal$NRO_ACCID
lm6_tr_semanal_mse <- MSE(predicted, actual) # MSE
lm6_tr_semanal_mae <- MAE(predicted, actual) # MAE
lm6_tr_semanal_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_tr_semanal_mse, lm6_tr_semanal_mae, lm6_tr_semanal_r2)
```

<div style="text-align: justify">
Para los datos de entrenamiento, se obtiene un MAE de `r lm6_tr_semanal_mae`, y un R2 de `r lm6_tr_semanal_r2`.
</div>

### 3.2.1.2 Predicción y Evaluación para los datos de Validación en el año 2019

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_val_semanal <- datos_val_2019 %>% group_by(FECHA, FESTIVIDAD, SEMANA, 
                                        CLASE, COMUNA) %>% count(name = "NRO_ACCID")

datos_lm6_val_pred_semanal <- datos_lm6_val_semanal[,-c(6)]
predicted <- round(predict(lm6_semanal, newdata=datos_lm6_val_pred_semanal, type="response"))
actual <- datos_lm6_val_semanal$NRO_ACCID
lm6_val_semanal_mse <- MSE(predicted, actual) # MSE
lm6_val_semanal_mae <- MAE(predicted, actual) # MAE
lm6_val_semanal_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_val_semanal_mse, lm6_val_semanal_mae, lm6_val_semanal_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2019, se obtiene un MAE de `r lm6_val_semanal_mae`, y un R2 de `r lm6_val_semanal_r2`. 
</div>

### 3.2.1.3 Comparación entre las métricas de entrenamiento y validación (2019)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm6_tr_semanal_mae
mae_vl = lm6_val_semanal_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación fue muy pequeña, de tan solo `r resultado`%; este dato nos ayuda a estar muy seguros de que no hay sobre entrenamiento. Con este modelo también conseguimos un MAE muy pequeño, cercano a 1.2, y el R2 cuadrado no disminuyó demasiado, pues sigue estando cerca del 70%. Por tanto, concluimos que este modelo sigue siendo adecuado para realizar predicciones a nivel semanal, según nuestro criterio de éxito.
</div>

### 3.2.1.4 Predicción y Evaluación para los datos de Validación en el año 2020

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_val_semanal <- datos_val_2020 %>% group_by(FECHA, FESTIVIDAD, SEMANA, 
                                        CLASE, COMUNA) %>% count(name = "NRO_ACCID")

datos_lm6_val_pred_semanal <- datos_lm6_val_semanal[,-c(6)]
predicted <- round(predict(lm6_semanal, newdata=datos_lm6_val_pred_semanal, type="response"))
actual <- datos_lm6_val_semanal$NRO_ACCID
lm6_val_semanal_mse <- MSE(predicted, actual) # MSE
lm6_val_semanal_mae <- MAE(predicted, actual) # MAE
lm6_val_semanal_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_val_semanal_mse, lm6_val_semanal_mae, lm6_val_semanal_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2020, se obtiene un MAE de `r lm6_val_semanal_mae`, y un R2 de `r lm6_val_semanal_r2`.
</div>

### 3.2.1.5 Comparación entre las métricas de entrenamiento y validación (2020)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm6_tr_semanal_mae
mae_vl = lm6_val_semanal_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación fue de `r resultado`%.
</div>

### 3.3 Mensual

<div style="text-align: justify">
Finalmente, evaluaremos el modelo 5 de manera mensual, para validar su eficacia en este caso.
</div>

### 3.3.1 Modelo seleccionado: modelo lineal generalizado, usando las variables clase y comuna

<div style="text-align: justify">
Veamos cómo se comporta este modelo para predecir la accidentalidad mensualmente. Las variables a usar son `FESTIVIDAD`, `MES`, `CLASE` y `COMUNA`.
</div>
```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_tr_mensual <- base_train %>% group_by(FECHA, FESTIVIDAD, MES, 
                                   CLASE, COMUNA) %>% count(name = "NRO_ACCID")
# lm6 <- lm(NRO_ACCID ~ FESTIVIDAD+DIA_SEMANA+CLASE+COMUNA,
#            data = datos_lm6_tr)
lm6_mensual <- glm(NRO_ACCID ~ FESTIVIDAD+MES+CLASE+COMUNA, family = "poisson",
           data = datos_lm6_tr_mensual)
```

### 3.3.2 Predicción y Evaluación para los datos de Entrenamiento

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_tr_pred_mensual <- datos_lm6_tr_mensual[,-c(6)]
predicted <- round(predict(lm6_mensual, newdata=datos_lm6_tr_pred_mensual, type="response"))
actual <- datos_lm6_tr_mensual$NRO_ACCID
lm6_tr_mensual_mse <- MSE(predicted, actual) # MSE
lm6_tr_mensual_mae <- MAE(predicted, actual) # MAE
lm6_tr_mensual_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_tr_mensual_mse, lm6_tr_mensual_mae, lm6_tr_mensual_r2)
```

<div style="text-align: justify">
Para los datos de entrenamiento, se obtiene un MAE de `r lm6_tr_mensual_mae`, y un R2 de `r lm6_tr_mensual_r2`.
</div>

### 3.3.3 Predicción y Evaluación para los datos de Validación en el año 2019

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_val_mensual <- datos_val_2019 %>% group_by(FECHA, FESTIVIDAD, MES, 
                                        CLASE, COMUNA) %>% count(name = "NRO_ACCID")

datos_lm6_val_pred_mensual <- datos_lm6_val_mensual[,-c(6)]
predicted <- round(predict(lm6_mensual, newdata=datos_lm6_val_pred_mensual, type="response"))
actual <- datos_lm6_val_mensual$NRO_ACCID
lm6_val_mensual_mse <- MSE(predicted, actual) # MSE
lm6_val_mensual_mae <- MAE(predicted, actual) # MAE
lm6_val_mensual_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_val_mensual_mse, lm6_val_mensual_mae, lm6_val_mensual_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2019, se obtiene un MAE de `r lm6_val_mensual_mae`, y un R2 de `r lm6_val_mensual_r2`.
</div>

### 3.3.4 Comparación entre las métricas de entrenamiento y validación (2019)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm6_tr_mensual_mae
mae_vl = lm6_val_mensual_mae
resultado =  print(mae_variation(mae_tr, mae_val))
```

<div style="text-align: justify">
La variación entre el MAE de entrenamiento y validación fue muy pequeña, de tan solo `r resultado`%; este dato nos ayuda a estar muy seguros de que no hay sobre entrenamiento. Con este modelo también conseguimos un MAE muy pequeño, cercano a 1.2, y el R2 cuadrado no disminuyó demasiado, pues sigue estando cerca del 70%. Por tanto, concluimos que este modelo sigue siendo adecuado para realizar predicciones a nivel semanal, según nuestro criterio de éxito.
</div>

### 3.3.5 Predicción y Evaluación para los datos de Validación en el año 2020

```{r message=FALSE, warning=FALSE, echo=F}
datos_lm6_val_mensual <- datos_val_2020 %>% group_by(FECHA, FESTIVIDAD, MES, 
                                        CLASE, COMUNA) %>% count(name = "NRO_ACCID")

datos_lm6_val_pred_mensual <- datos_lm6_val_mensual[,-c(6)]
predicted <- round(predict(lm6_mensual, newdata=datos_lm6_val_pred_mensual, type="response"))
actual <- datos_lm6_val_mensual$NRO_ACCID
lm6_val_mensual_mse <- MSE(predicted, actual) # MSE
lm6_val_mensual_mae <- MAE(predicted, actual) # MAE
lm6_val_mensual_r2 <- R2_Score(predicted, actual) # R2
sprintf("MSE: %f, MAE: %f, R2: %f", lm6_val_mensual_mse, lm6_val_mensual_mae, lm6_val_mensual_r2)
```

<div style="text-align: justify">
Para los datos de validación del año 2020, se obtiene un MAE de `r lm6_val_mensual_mae`, y un R2 de `r lm6_val_mensual_r2`.
</div>

### 3.3.6 Comparación entre las métricas de entrenamiento y validación (2020)

```{r message=FALSE, warning=FALSE, echo=F}
mae_tr = lm6_tr_mensual_mae
mae_vl = lm6_val_mensual_mae
resultado = print(mae_variation(mae_tr, mae_val))
```

```{r}
save(file="lm6_s.RData",lm6_semanal)
```

```{r}
save(file="lm6_m.RData",lm6_mensual)
```